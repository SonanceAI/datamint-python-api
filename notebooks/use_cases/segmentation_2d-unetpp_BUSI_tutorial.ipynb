{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332c2c31",
   "metadata": {},
   "source": [
    "# Medical Image Segmentation with UNet++ and Datamint\n",
    "\n",
    "This notebook demonstrates how to build an end-to-end **semantic segmentation** pipeline using **Datamint** and the **BUSI** (Breast Ultrasound Images) dataset with a **UNet++** architecture.\n",
    "\n",
    "## Overview\n",
    "\n",
    "You will learn how to:\n",
    "- **Set up a Datamint project** for managing medical imaging data\n",
    "- **Upload ultrasound images and segmentation masks** to Datamint\n",
    "- **Build a custom PyTorch Dataset** that integrates with Datamint\n",
    "- **Implement UNet++ with combined loss functions** (CrossEntropy + Dice)\n",
    "- **Train the model** using PyTorch Lightning with MLflow tracking\n",
    "- **Deploy the model** for inference using Datamint's model serving\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup: Create Project and Initialize API](#1-setup-create-project-and-initialize-api)\n",
    "2. [Dataset Preparation: Download and Upload BUSI](#2-dataset-preparation-download-and-upload-busi)\n",
    "3. [Custom PyTorch Dataset](#3-custom-pytorch-dataset)\n",
    "4. [Model Architecture: UNet++ with Combined Loss](#4-model-architecture-unet-with-combined-loss)\n",
    "5. [Training with MLflow Integration](#5-training-with-mlflow-integration)\n",
    "6. [Visualization and Evaluation](#6-visualization-and-evaluation)\n",
    "7. [Model Deployment](#7-model-deployment)\n",
    "\n",
    "## Required Dependencies\n",
    "\n",
    "```bash\n",
    "pip install datamint segmentation-models-pytorch albumentations\n",
    "```\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The **BUSI** (Breast Ultrasound Images) dataset contains ultrasound images of breast cancer with corresponding segmentation masks.\n",
    "Three classes: benign, malignant, and normal tissues.\n",
    "\n",
    "Dataset reference:\n",
    "> Al-Dhabyani W, Gomaa M, Khaled H, Fahmy A. Dataset of breast ultrasound images. Data in Brief. 2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50baf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamint import Api\n",
    "from datamint.mlflow import set_project\n",
    "\n",
    "PROJECT_NAME = \"UNetPP_Segmentation_Tutorial\"\n",
    "api = Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf543a",
   "metadata": {},
   "source": [
    "## 1. Setup: Create Project and Initialize API\n",
    "\n",
    "In this section, we will:\n",
    "- Create a new Datamint project (or retrieve an existing one)\n",
    "- Set up the MLflow project context for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = api.projects.get_by_name(PROJECT_NAME)\n",
    "if proj is None:\n",
    "    print(f\"Creating project '{PROJECT_NAME}'\")\n",
    "    proj = api.projects.create(\n",
    "        name=PROJECT_NAME,\n",
    "        description=\"Tutorial project for UNet++ segmentation on BUSI dataset\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"Using existing project '{PROJECT_NAME}'\")\n",
    "\n",
    "set_project(PROJECT_NAME)  # Important for proper experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad930c7",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation: Download and Upload BUSI\n",
    "\n",
    "In this section, we will:\n",
    "- Download the BUSI dataset\n",
    "- Upload ultrasound images to Datamint\n",
    "- Upload corresponding segmentation masks\n",
    "- Create train/val/test splits\n",
    "\n",
    "### 2.1 Download BUSI Dataset\n",
    "\n",
    "The BUSI dataset is available from Kaggle. For this tutorial, we'll use the breast ultrasound images dataset.\n",
    "\n",
    "> Al-Dhabyani W, Gomaa M, Khaled H, Fahmy A. Dataset of breast ultrasound images. Data in Brief. 2020 Feb;28:104863. DOI: 10.1016/j.dib.2019.104863."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e40043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "BUSI_URL = \"https://www.kaggle.com/api/v1/datasets/download/sabahesaraki/breast-ultrasound-images-dataset\"\n",
    "DATA_DIR = Path(\"/tmp/BUSI_dataset\") # Change this path as needed\n",
    "\n",
    "if not DATA_DIR.exists():\n",
    "    print(\"Downloading BUSI dataset...\")\n",
    "\n",
    "    # Download the dataset\n",
    "    response = requests.get(BUSI_URL, stream=True)\n",
    "    response.raise_for_status()\n",
    "    zip_path = DATA_DIR / \"Dataset_BUSI.zip\"\n",
    "\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    with open(zip_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    print(\"Extracting...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(DATA_DIR)\n",
    "\n",
    "    os.remove(zip_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(f\"Dataset already exists at {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d497e3a",
   "metadata": {},
   "source": [
    "### 2.2 Explore Dataset Structure\n",
    "\n",
    "The BUSI dataset contains folders for each class:\n",
    "- **benign**: Images and masks for benign cases\n",
    "- **malignant**: Images and masks for malignant cases\n",
    "- **normal**: Images and masks for normal cases\n",
    "\n",
    "```\n",
    "Dataset_BUSI/\n",
    "â”œâ”€â”€ benign/\n",
    "â”‚   â”œâ”€â”€ benign (1).png\n",
    "â”‚   â”œâ”€â”€ benign (1)_mask.png\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”œâ”€â”€ malignant/\n",
    "â”‚   â””â”€â”€ ...\n",
    "â””â”€â”€ normal/\n",
    "    â””â”€â”€ ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e6673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find image and label paths\n",
    "base_dir = DATA_DIR / \"Dataset_BUSI_with_GT\"\n",
    "classes = [\"benign\", \"malignant\", \"normal\"]\n",
    "\n",
    "image_paths = []\n",
    "label_paths = []\n",
    "\n",
    "for cls in classes:\n",
    "    cls_dir = base_dir / cls\n",
    "    # Images are files that don't contain \"_mask\"\n",
    "    cls_images = sorted([p for p in cls_dir.glob(\"*.png\") if \"_mask\" not in p.name])\n",
    "    for img_p in cls_images:\n",
    "        # Find corresponding mask (taking the first one if multiple exist)\n",
    "        mask_p = cls_dir / f\"{img_p.stem}_mask.png\"\n",
    "        if mask_p.exists():\n",
    "            image_paths.append(img_p)\n",
    "            label_paths.append(mask_p)\n",
    "\n",
    "print(f\"Found {len(image_paths)} ultrasound images\")\n",
    "print(f\"Found {len(label_paths)} segmentation masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d420790c",
   "metadata": {},
   "source": [
    "### 2.3 Define Classes\n",
    "\n",
    "We map class ids to class names for segmentation.\n",
    "We need this mapping because training labels are stored as integers (not strings) in the masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a254a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class mapping for segmentation. \n",
    "# Class 0 is background (no tumor)\n",
    "CLASS_NAMES = {\n",
    "    0: \"background\",\n",
    "    1: \"benign\",\n",
    "    2: \"malignant\",\n",
    "}\n",
    "\n",
    "CLASS_NAME_TO_LABEL = {v: k for k, v in CLASS_NAMES.items()}\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464994f5",
   "metadata": {},
   "source": [
    "### 2.4 Upload Images to Datamint\n",
    "\n",
    "We upload each ultrasound image as a resource with appropriate tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0165e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload images to Datamint\n",
    "uploaded_resources = api.resources.upload_resources(\n",
    "    [str(p) for p in image_paths],\n",
    "    tags=['busi', 'ultrasound', 'breast'],\n",
    "    publish_to=proj,\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Uploaded {len(uploaded_resources)} images to Datamint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea53fe",
   "metadata": {},
   "source": [
    "### 2.5 Upload Segmentation Masks\n",
    "\n",
    "Now we upload the corresponding segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c233a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Get resources from project\n",
    "all_resources = list(api.resources.get_list(project_name=PROJECT_NAME, tags=['busi']))\n",
    "filename_to_resource = {r.filename: r for r in all_resources}\n",
    "\n",
    "# Upload segmentation masks\n",
    "for img_path, label_path in tqdm(zip(image_paths, label_paths), total=len(image_paths)):\n",
    "    if 'normal' in img_path.parent.name:\n",
    "        # Skip normal images (no lesions)\n",
    "        continue\n",
    "    resource = filename_to_resource[img_path.name]\n",
    "    \n",
    "    # Determine class from file path. Example ``img_path``: 'Dataset_BUSI_with_GT/benign/BUSI_123.png'\n",
    "    cls_name = img_path.parent.name # 'benign' or 'malignant'\n",
    "    \n",
    "    api.annotations.upload_segmentations(\n",
    "        resource=resource,\n",
    "        file_path=label_path,\n",
    "        name=cls_name,\n",
    "        imported_from=\"Original GT BUSI Dataset\", # source of the masks. Arbitrary string\n",
    "    )\n",
    "\n",
    "print(\"Segmentation masks uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37aa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify uploads - inspect a sample resource\n",
    "sample_resource = api.resources.get_list(project_name=PROJECT_NAME, limit=1)[0]\n",
    "sample_annotations = api.annotations.get_list(resource=sample_resource, annotation_type='segmentation')\n",
    "\n",
    "print(f\"Resource: {sample_resource.filename}\")\n",
    "print(f\"  Number of segmentation annotations: {len(sample_annotations)}\")\n",
    "if sample_annotations:\n",
    "    print(f\"  First annotation: {sample_annotations[0].asdict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4562af",
   "metadata": {},
   "source": [
    "### 2.6 Create Train/Validation/Test Splits\n",
    "\n",
    "We split the dataset into three subsets using tags for reproducibility.\n",
    "\n",
    "| Split | Percentage | Purpose |\n",
    "|-------|------------|---------|\n",
    "| Train | 70% | Model training |\n",
    "| Validation | 15% | Hyperparameter tuning, early stopping |\n",
    "| Test | 15% | Final model evaluation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dadab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get all resources\n",
    "all_resources = list(api.resources.get_list(project_name=PROJECT_NAME, tags=['busi']))\n",
    "all_resources.sort(key=lambda r: r.filename)\n",
    "\n",
    "# Shuffle with fixed seed\n",
    "random.seed(42)\n",
    "random.shuffle(all_resources)\n",
    "\n",
    "# Split ratios\n",
    "n_total = len(all_resources)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "train_resources = all_resources[:n_train]\n",
    "val_resources = all_resources[n_train:n_train + n_val]\n",
    "test_resources = all_resources[n_train + n_val:]\n",
    "\n",
    "print(f\"Total resources: {n_total}\")\n",
    "print(f\"Training: {len(train_resources)}\")\n",
    "print(f\"Validation: {len(val_resources)}\")\n",
    "print(f\"Test: {len(test_resources)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f94f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply split tags to resources\n",
    "api.resources.add_tags(train_resources, ['split:train'])\n",
    "api.resources.add_tags(val_resources, ['split:val'])\n",
    "api.resources.add_tags(test_resources, ['split:test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6fb528",
   "metadata": {},
   "source": [
    "## 3. Custom PyTorch Dataset\n",
    "\n",
    "In this section, we'll build a PyTorch Dataset that:\n",
    "- Fetches images and segmentation masks from Datamint\n",
    "- Applies normalization\n",
    "- Uses Albumentations for data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef441a",
   "metadata": {},
   "source": [
    "### 3.1 Define Data Transforms\n",
    "\n",
    "We use [Albumentations](https://albumentations.ai/) for image and mask augmentation. The key is that augmentations are applied **consistently** to both image and mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adbc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Image size for UNet++ (should be divisible by 32 for encoder-decoder architectures)\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "# Training transforms with augmentation\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.ElasticTransform(alpha=50, sigma=5, p=0.3),\n",
    "    A.GridDistortion(num_steps=5, distort_limit=0.2, p=0.3),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.Normalize(0,1),  # Normalize to [0, 1] by dividing by 255\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.Normalize(0,1), # Normalize to [0, 1] by dividing by 255\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04637e73",
   "metadata": {},
   "source": [
    "### 3.2 Implement Custom PyTorch Dataset\n",
    "\n",
    "The `MedicalSegmentationDataset` class handles:\n",
    "- Loading images from Datamint\n",
    "- Applying preprocessing and augmentation\n",
    "- Returning image-mask pairs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class MedicalSegmentationDataset(Dataset[dict]): # dataset that returns dicts\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: str | None = None,\n",
    "        transforms =  None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Medical Segmentation Dataset for BUSI dataset stored in Datamint.\n",
    "\n",
    "        Args:\n",
    "            split (str | None): One of 'train', 'val', 'test', or None for all data.\n",
    "            transforms: Albumentations transforms to apply.\n",
    "            resources (list[Resource] | None): Optional pre-fetched list of resources. If None, fetches from Datamint based on split.\n",
    "            inference_mode (bool): If True, dataset is used for inference (no masks).\n",
    "        \"\"\"\n",
    "        self.transforms = transforms\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        \n",
    "        self.resources = api.resources.get_list(\n",
    "            project_name=PROJECT_NAME,\n",
    "            tags=[f'split:{split}'] if split else None,\n",
    "        )\n",
    "            \n",
    "        all_annotations = api.annotations.get_list(\n",
    "            resource=self.resources,\n",
    "            annotation_type='segmentation'\n",
    "        )\n",
    "    \n",
    "        self.resource_annotations = defaultdict(list) # just mapping resource_id -> [annotations]\n",
    "        for ann in all_annotations:\n",
    "            self.resource_annotations[ann.resource_id].append(ann)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.resources)\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> dict:\n",
    "        resource = self.resources[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = resource.fetch_file_data(auto_convert=True, use_cache=True)\n",
    "        original_width = image.width\n",
    "        original_height = image.height\n",
    "        # image is a PIL.Image object\n",
    "        image = np.array(image) # image.shape: (H, W, 3) or (H, W)\n",
    "        \n",
    "        # Convert grayscale to RGB if needed\n",
    "        if image.ndim == 2:\n",
    "            image = np.stack([image] * 3, axis=-1)\n",
    "        \n",
    "        # Load mask if not in inference mode\n",
    "        annotations = self.resource_annotations[resource.id]\n",
    "        if not annotations or 'normal' in resource.filename:\n",
    "            mask = np.zeros((original_height, original_width), dtype=np.int64)\n",
    "        else:\n",
    "            if len(annotations) > 1:\n",
    "                print(f\"Warning: Resource {resource.filename} has multiple annotations. Using the first one.\")\n",
    "            mask = np.array(annotations[0].fetch_file_data(use_cache=True)).astype(np.int64)\n",
    "            # Convert binary mask {0, 255} to class indices {0, 1 or 2}\n",
    "            # Determine class from filename\n",
    "            if 'benign' in resource.filename:\n",
    "                mask = (mask > 0).astype(np.int64)  # 0 -> 0 (background), 255 -> 1 (benign)\n",
    "            elif 'malignant' in resource.filename:\n",
    "                mask = (mask > 0).astype(np.int64) * 2  # 0 -> 0 (background), 255 -> 2 (malignant)\n",
    "        \n",
    "        if self.transforms:\n",
    "            if mask is not None:\n",
    "                transformed = self.transforms(image=image, mask=mask)\n",
    "                image = transformed['image']\n",
    "                mask = transformed['mask'].long()\n",
    "            else:\n",
    "                transformed = self.transforms(image=image)\n",
    "                image = transformed['image']\n",
    "        else:\n",
    "            # Fallback if no transforms provided\n",
    "            image = torch.from_numpy(image).float() / 255.0\n",
    "            if image.ndim == 2:\n",
    "                image = image.unsqueeze(0)\n",
    "            if mask is not None:\n",
    "                mask = torch.from_numpy(mask).long()\n",
    "        \n",
    "        res = {\n",
    "            \"image\": image, \n",
    "            \"mask\": mask,\n",
    "            \"filename\": resource.filename,\n",
    "            'original_width': original_width,\n",
    "            'original_height': original_height\n",
    "        }\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "\n",
    "train_dataset = MedicalSegmentationDataset(\n",
    "    split='train',\n",
    "    transforms=train_transforms,\n",
    ")\n",
    "\n",
    "train_dataset[0].keys()  # Fetch first sample to test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2279a59c",
   "metadata": {},
   "source": [
    "### 3.4 Create DataLoaders\n",
    "\n",
    "Now we instantiate datasets and dataloaders for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168737d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 16 # Adjust based on your GPU memory\n",
    "NUM_WORKERS = 4  # Adjust based on your CPU cores\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "print(\"Building training dataset...\")\n",
    "train_dataset = MedicalSegmentationDataset(\n",
    "    split='train',\n",
    "    transforms=train_transforms,\n",
    ")\n",
    "print(f\"  Training samples (slices): {len(train_dataset)}\")\n",
    "\n",
    "print(\"Building validation dataset...\")\n",
    "val_dataset = MedicalSegmentationDataset(\n",
    "    split='val',\n",
    "    transforms=val_transforms,\n",
    ")\n",
    "print(f\"  Validation samples (slices): {len(val_dataset)}\")\n",
    "\n",
    "print(\"Building test dataset...\")\n",
    "test_dataset = MedicalSegmentationDataset(\n",
    "    split='test',\n",
    "    transforms=val_transforms,\n",
    ")\n",
    "print(f\"  Test samples (slices): {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6352de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample batch\n",
    "from datamint.utils.visualization import show, draw_masks\n",
    "\n",
    "\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(f\"Batch image shape: {sample_batch['image'].shape}\")  # (B, C, H, W)\n",
    "print(f\"Batch mask shape: {sample_batch['mask'].shape}\")    # (B, H, W)\n",
    "\n",
    "# Plot first 2 samples\n",
    "for i in range(2):\n",
    "    img = sample_batch['image'][i]\n",
    "    mask = sample_batch['mask'][i]\n",
    "    print(f\"Filename: {sample_batch['filename'][i]}\")\n",
    "    img_with_mask = draw_masks(img, mask)\n",
    "    show(img_with_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af0231",
   "metadata": {},
   "source": [
    "## 4. Model Architecture: UNet++ with Combined Loss\n",
    "\n",
    "In this section, we'll implement:\n",
    "- **UNet++** architecture using `segmentation_models_pytorch`\n",
    "- **Dice Loss** for handling class imbalance\n",
    "- **Combined Loss** (CrossEntropy + Dice) for better segmentation\n",
    "- **Lightning Module** for training\n",
    "\n",
    "### 4.1 UNet++ Architecture\n",
    "\n",
    "UNet++ is an improved version of U-Net with nested dense skip connections. This architecture enhances feature propagation and reduces the semantic gap between the encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590980e1",
   "metadata": {},
   "source": [
    "### 4.2 Custom Loss Functions\n",
    "\n",
    "For semantic segmentation, we use a combination of:\n",
    "- **CrossEntropyLoss**: Standard classification loss per pixel\n",
    "- **Dice Loss**: Optimizes the Dice coefficient directly, handles class imbalance better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.segmentation import DiceScore\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined CrossEntropy and Dice Loss.\n",
    "    \n",
    "    This combination provides:\n",
    "    - CrossEntropy: Pixel-level classification accuracy\n",
    "    - Dice: Global overlap optimization, handles class imbalance\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of segmentation classes\n",
    "        ce_weight: Weight for CrossEntropy loss\n",
    "        dice_weight: Weight for Dice loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        ce_weight: float = 1.0,\n",
    "        dice_weight: float = 1.0,\n",
    "        class_weights: torch.Tensor | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        self.dicescore = DiceScore(num_classes=num_classes, \n",
    "                                   average='macro',\n",
    "                                   input_format=\"mixed\")\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: Predictions (B, C, H, W) - logits\n",
    "            target: Ground truth (B, H, W) - class indices\n",
    "            \n",
    "        Returns:\n",
    "            Combined loss (scalar)\n",
    "        \"\"\"\n",
    "        ce = self.ce_loss(pred, target.long())\n",
    "        dice = 1 - self.dicescore(F.softmax(pred, dim=1), target)\n",
    "        \n",
    "        return self.ce_weight * ce + self.dice_weight * dice\n",
    "\n",
    "\n",
    "# Quick test\n",
    "dummy_pred = torch.randn(2, NUM_CLASSES+1, 64, 64) # logits\n",
    "dummy_target = torch.randint(0, NUM_CLASSES+1, (2, 64, 64))\n",
    "\n",
    "loss_fn = CombinedLoss(num_classes=NUM_CLASSES+1)\n",
    "loss = loss_fn(dummy_pred, dummy_target)\n",
    "print(f\"Test loss value: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b31e6",
   "metadata": {},
   "source": [
    "### 4.3 UNet++ Lightning Module\n",
    "\n",
    "We wrap UNet++ in a PyTorch Lightning module for clean training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import lightning as L\n",
    "from torchmetrics.segmentation import MeanIoU, GeneralizedDiceScore\n",
    "\n",
    "\n",
    "class UNetPPModule(L.LightningModule):\n",
    "    \"\"\"PyTorch Lightning module for UNet++ segmentation.\n",
    "    \n",
    "    This module handles:\n",
    "    - Model architecture (UNet++ with pretrained encoder)\n",
    "    - Combined loss function (CrossEntropy + Dice)\n",
    "    - Metrics tracking (IoU, Dice)\n",
    "    - Optimizer configuration with learning rate scheduling\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of segmentation classes (including background)\n",
    "        encoder_name: Name of the encoder backbone (e.g., 'resnet34', 'efficientnet-b0')\n",
    "        learning_rate: Initial learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        encoder_name: str = 'resnet34',\n",
    "        learning_rate: float = 1e-4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # Save hyperparameters for logging\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # UNet++ model from segmentation_models_pytorch\n",
    "        self.model = smp.UnetPlusPlus(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights='imagenet',\n",
    "            in_channels=3,  # RGB input (we repeat grayscale to 3 channels)\n",
    "            classes=num_classes,\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = CombinedLoss(\n",
    "            num_classes=num_classes,\n",
    "            ce_weight=1.0,\n",
    "            dice_weight=1.0,\n",
    "        )\n",
    "\n",
    "        # Metrics for each split\n",
    "        # Note: MeanIoU expects predictions and targets as one-hot encoded by default.\n",
    "        #   Therefore, we need to change expected input_format.\n",
    "        self.iou_metrics = {\n",
    "            'train': MeanIoU(num_classes=num_classes, input_format='index'),\n",
    "            'val': MeanIoU(num_classes=num_classes, input_format='index'),\n",
    "            'test': MeanIoU(num_classes=num_classes, input_format='index'),\n",
    "        }\n",
    "        # register the iou metrics:\n",
    "        for stage, metric in self.iou_metrics.items():\n",
    "            self.add_module(f\"{stage}_mean_iou\", metric)\n",
    "\n",
    "        self.dice_metrics = {\n",
    "            'train': GeneralizedDiceScore(num_classes=num_classes, input_format='index'),\n",
    "            'val': GeneralizedDiceScore(num_classes=num_classes, input_format='index'),\n",
    "            'test': GeneralizedDiceScore(num_classes=num_classes, input_format='index'),\n",
    "        }\n",
    "        # register the dice metrics:\n",
    "        for stage, metric in self.dice_metrics.items():\n",
    "            self.add_module(f\"{stage}_dice_score\", metric)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through UNet++.\"\"\"\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _common_step(self, batch: dict, stage: str) -> torch.Tensor:\n",
    "        \"\"\"Common step for train/val/test.\n",
    "        \n",
    "        Args:\n",
    "            batch: Dictionary with 'image' and 'mask' tensors\n",
    "            stage: One of 'train', 'val', 'test'\n",
    "            \n",
    "        Returns:\n",
    "            Loss tensor\n",
    "        \"\"\"\n",
    "        images = batch['image']\n",
    "        masks = batch['mask'] # input format: index-based. shape: (B, H, W)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self(images)  # (B, C, H, W)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.criterion(logits, masks)\n",
    "        \n",
    "        # Get predictions (class indices)\n",
    "        preds = torch.argmax(logits, dim=1)  # (B, H, W)\n",
    "        \n",
    "        # Update metrics\n",
    "        if stage is not None:\n",
    "            self.iou_metrics[stage].update(preds, masks)\n",
    "            self.dice_metrics[stage].update(preds, masks)\n",
    "            self.log(f'{stage}/loss', loss, on_step=(stage == 'train'), on_epoch=True, prog_bar=True, batch_size=len(images))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        return self._common_step(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        return self._common_step(batch, 'val')\n",
    "    \n",
    "    def test_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        return self._common_step(batch, 'test')\n",
    "    \n",
    "    def predict_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        images = batch['image']\n",
    "        logits = self(images)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return preds\n",
    "    \n",
    "    def _common_epoch_end(self, stage: str):\n",
    "        iou = self.iou_metrics[stage]\n",
    "        dice = self.dice_metrics[stage]\n",
    "        self.log(f'{stage}/iou', iou.compute(), prog_bar=True)\n",
    "        self.log(f'{stage}/dice', dice.compute())\n",
    "        iou.reset()\n",
    "        dice.reset()\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self._common_epoch_end('train')\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self._common_epoch_end('val')\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self._common_epoch_end('test')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val/loss',\n",
    "                'interval': 'epoch',\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = UNetPPModule(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    encoder_name='resnet34',  # Lightweight encoder for faster training\n",
    "    learning_rate=1e-4,\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "with torch.inference_mode():\n",
    "    sample_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    sample_output = model(sample_input)\n",
    "    print(f\"Output shape: {sample_output.shape}\")\n",
    "\n",
    "    # test loss computation\n",
    "    sample_target = train_dataloader.dataset[-1]['mask']  # (1, H, W)\n",
    "    sample_loss = model.criterion(sample_output, sample_target.unsqueeze(0))\n",
    "    print(f\"Sample loss: {sample_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fef5b4",
   "metadata": {},
   "source": [
    "## 5. Training with MLflow Integration\n",
    "\n",
    "In this section, we'll:\n",
    "- Configure MLflow for experiment tracking\n",
    "- Set up model checkpointing with automatic registration\n",
    "- Train the model with early stopping\n",
    "- Monitor progress via Datamint dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326bc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamint.mlflow.lightning.callbacks import MLFlowModelCheckpoint\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "# Ensure project context is set\n",
    "set_project(PROJECT_NAME)\n",
    "\n",
    "# MLflow checkpoint callback\n",
    "# This callback automatically:\n",
    "# - Saves the best model based on validation IoU\n",
    "# - Registers the model in MLflow Model Registry after testing\n",
    "checkpoint_callback = MLFlowModelCheckpoint(\n",
    "    monitor=\"val/iou\",                    # Metric to monitor\n",
    "    mode=\"max\",                           # Save when metric increases\n",
    "    save_top_k=1,                         # Keep only the best model\n",
    "    filename=\"best_unetpp\",               # Checkpoint filename\n",
    "    save_weights_only=True,              # Save full model state\n",
    "    register_model_name=PROJECT_NAME,     # Name in Model Registry\n",
    "    register_model_on='test',             # Register after test evaluation\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val/iou\",\n",
    "    mode=\"max\",\n",
    "    patience=10                          # Stop if no improvement for 10 epochs\n",
    ")\n",
    "\n",
    "# MLflow logger for experiment tracking\n",
    "mlflow_logger = MLFlowLogger(\n",
    "    experiment_name=f\"{PROJECT_NAME}_training\",\n",
    "    run_name=\"unetpp_resnet34_busi_metrics_fixed\",\n",
    ")\n",
    "\n",
    "print(\"Training callbacks configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d4f80",
   "metadata": {},
   "source": [
    "### 5.1 Start Training\n",
    "\n",
    "We use PyTorch Lightning's Trainer for clean, scalable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab7f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=20,                        # Maximum training epochs\n",
    "    logger=mlflow_logger,                 # MLflow logging\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    accelerator='auto',                   # Auto-detect GPU/CPU\n",
    "    # precision='16-mixed',                 # Mixed precision for faster training, if supported\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"ðŸš€ Starting training...\")\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d829c3",
   "metadata": {},
   "source": [
    "### 5.2 Monitor Training Progress\n",
    "\n",
    "While training runs, you can monitor progress in two ways:\n",
    "1. **Terminal output**: Loss and metrics per epoch\n",
    "2. **Datamint Dashboard**: Visual experiment tracking\n",
    "\n",
    "Run `proj.show()` to open the project dashboard in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59751c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Datamint project dashboard\n",
    "proj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3d3607",
   "metadata": {},
   "source": [
    "### 5.3 Evaluate on Test Set\n",
    "\n",
    "After training, evaluate the model on the test set to get final metrics.\n",
    "This also triggers model registration in MLflow (due to `register_model_on='test'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set and register model\n",
    "print(\"ðŸ” Evaluating on test set...\")\n",
    "test_results = trainer.test(dataloaders=test_dataloader)\n",
    "\n",
    "print(f\"Best model checkpoint: {checkpoint_callback.best_model_path}\")\n",
    "print(f\"Best validation IoU: {checkpoint_callback.best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916bf0fc",
   "metadata": {},
   "source": [
    "## 6. Visualization and Evaluation\n",
    "\n",
    "In this section, we'll:\n",
    "- Visualize predictions vs ground truth\n",
    "- Create overlay visualizations\n",
    "- Analyze per-class performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f7b49e",
   "metadata": {},
   "source": [
    "### 6.1 Visualize Predictions\n",
    "\n",
    "Let's visualize model predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset):\n",
    "    \"\"\"Visualize model predictions compared to ground truth.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataset: Dataset to sample from\n",
    "        indices: Specific indices to visualize (optional)\n",
    "        device: Device for inference\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    idx = np.random.choice(len(dataset), 1, replace=False)[0]\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        sample = dataset[idx]\n",
    "        image = sample['image'].unsqueeze(0).to(model.device)\n",
    "        mask_gt = sample['mask'] # shape: (H, W)\n",
    "        \n",
    "        logits = model(image) # shape: (1, #classes, H, W)\n",
    "        mask_pred = torch.argmax(logits, dim=1).squeeze(0).cpu() # shape: (H, W)\n",
    "\n",
    "        overlay_mask = draw_masks(image.squeeze(0).cpu(), torch.stack([mask_gt, mask_pred]), alpha=0.5)\n",
    "        show(overlay_mask)\n",
    "\n",
    "        yp = (mask_pred == 1).sum().item() # predicted positives\n",
    "        yg = (mask_gt == 1).sum().item() # ground truth positives\n",
    "        tp = ((mask_pred == 1) & (mask_gt == 1)).sum().item() # true positives\n",
    "        iou = tp / (yp + yg - tp) if (yp + yg - tp) > 0 else 1.0\n",
    "        print(f\"IoU: {iou:.1%}\")\n",
    "\n",
    "visualize_predictions(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb374d2",
   "metadata": {},
   "source": [
    "## 7. Model Deployment\n",
    "\n",
    "For production use, we wrap our model in a **Datamint Model Adapter**. This adapter:\n",
    "- Standardizes input/output format\n",
    "- Handles resource loading from Datamint\n",
    "- Enables deployment via MLflow Model Serving or Datamint's inference API\n",
    "- Returns structured `ImageSegmentation` annotations\n",
    "\n",
    "### 7.1 Create Datamint Model Adapter\n",
    "\n",
    "The `DatamintModel` base class provides a consistent interface for model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53379296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamint.mlflow.flavors.model import DatamintModel\n",
    "from datamint.entities.annotations import ImageSegmentation\n",
    "from datamint.entities import Resource\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing_extensions import override\n",
    "\n",
    "class UNetPPSegmentationAdapter(DatamintModel):\n",
    "    \"\"\"Datamint adapter for UNet++ segmentation model deployment.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            mlflow_torch_models_uri={\n",
    "                'unetpp': f'models:/{PROJECT_NAME}/latest' # URI in MLflow Model Registry. An efficient way to link your new model to this one\n",
    "            }, \n",
    "            settings={'need_gpu': True}\n",
    "        )\n",
    "        self.class_names = CLASS_NAMES\n",
    "    \n",
    "    @override\n",
    "    def predict_image(self, model_input: list[Resource], **kwargs):\n",
    "        pytorch_model = self.get_mlflow_torch_models()['unetpp']\n",
    "        pytorch_model.eval()\n",
    "        \n",
    "        # Use Lightning Fabric for device management, instead of L.Trainer. Lightweight and perfect for inference.\n",
    "        fabric = L.Fabric(accelerator=self.inference_device)\n",
    "        pytorch_model = fabric.setup_module(pytorch_model)\n",
    "\n",
    "        all_predictions = []\n",
    "        with torch.inference_mode():\n",
    "            for res in model_input:\n",
    "                image = res.fetch_file_data(auto_convert=True, use_cache=True)\n",
    "                # image is a PIL.Image object\n",
    "                original_width = image.width\n",
    "                original_height = image.height\n",
    "\n",
    "                image = np.array(image)\n",
    "                image_tensor = val_transforms(image=image)['image'].to(fabric.device)  # (3, H, W)\n",
    "                logits = pytorch_model(image_tensor.unsqueeze(0)) # unsqueeze to (1, 3, H, W)\n",
    "                pred = torch.argmax(logits, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "                # Implement here any post-processing if desired\n",
    "                # reshape prediction to original size\n",
    "                pred = cv2.resize(pred.astype(np.uint8), \n",
    "                                  (original_width, original_height), \n",
    "                                  interpolation=cv2.INTER_NEAREST)\n",
    "                annotations = []\n",
    "                for class_idx, class_name in self.class_names.items():\n",
    "                    if class_idx == 0:  # Skip background class\n",
    "                        continue\n",
    "                    class_mask = (pred == class_idx).astype(np.uint8) * 255  # Convert to {0, 255} for visualization\n",
    "                    if class_mask.any(): # at least one pixel\n",
    "                        pred_ann = ImageSegmentation(name=class_name, mask=class_mask)\n",
    "                        annotations.append(pred_ann)\n",
    "                all_predictions.append(annotations)        \n",
    "\n",
    "        return all_predictions\n",
    "\n",
    "adapter = UNetPPSegmentationAdapter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4d082",
   "metadata": {},
   "source": [
    "### 7.2 Log the Adapter to MLflow\n",
    "\n",
    "We log the adapter model to MLflow, making it available for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamint.mlflow.flavors import datamint_flavor\n",
    "from mlflow import set_experiment\n",
    "import mlflow\n",
    "from datamint.mlflow import set_project\n",
    "\n",
    "# Set project and experiment context\n",
    "set_project(PROJECT_NAME)\n",
    "set_experiment(f'{PROJECT_NAME}_deployment') # Arbitrary experiment name. Create or use an existing one.\n",
    "\n",
    "adapter = UNetPPSegmentationAdapter()\n",
    "\n",
    "# Log the adapter to MLflow\n",
    "ADAPTED_MODEL_NAME = f\"{PROJECT_NAME}_adapted\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"unetpp_segmentation_adapter\"): # Create a new MLflow run. You can use an existing one as well\n",
    "    model_info = datamint_flavor.log_model(\n",
    "        adapter,\n",
    "        registered_model_name=ADAPTED_MODEL_NAME,\n",
    "    )\n",
    "\n",
    "print(f\"âœ… Adapter logged successfully!\")\n",
    "print(f\"Model URI: {model_info.model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dcee15",
   "metadata": {},
   "source": [
    "### 7.3 Test Local Inference\n",
    "\n",
    "Before deploying, verify the adapter works correctly with local inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Load the registered adapter model\n",
    "loaded_model = mlflow.pyfunc.load_model(f'models:/{ADAPTED_MODEL_NAME}/latest')\n",
    "\n",
    "# Test with a resource from the test set\n",
    "test_resource = test_dataset.resources[-1]\n",
    "print(f\"Testing with: {test_resource.filename}\")\n",
    "\n",
    "# Run prediction\n",
    "predictions = loaded_model.predict([test_resource])\n",
    "\n",
    "print(f\"\\nâœ… Prediction successful!\")\n",
    "print(f\"Number of annotations: {len(predictions[0])}\")\n",
    "for ann in predictions[0]:\n",
    "    n_pixels = (ann.mask > 0).sum()\n",
    "    print(f\"  - {ann.name}: {n_pixels} pixels ({n_pixels / (ann.mask.size) :.1%} of the image)\")\n",
    "\n",
    "print('Ground truth:')\n",
    "gt_annotations = api.annotations.get_list(\n",
    "    resource=test_resource,\n",
    "    annotation_type='segmentation'\n",
    ")\n",
    "for ann in gt_annotations:\n",
    "    mask = np.array(ann.fetch_file_data(use_cache=True))\n",
    "    n_pixels = (mask > 0).sum()\n",
    "    print(f\"  - {ann.name}: {n_pixels} pixels ({n_pixels / (mask.size) :.1%} of the image)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf920e",
   "metadata": {},
   "source": [
    "### 7.4 Deploy to Datamint Server\n",
    "\n",
    "Start a deployment job to serve the model via Datamint's inference API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start deployment job\n",
    "job = api.deploy.start(\n",
    "    model_name=ADAPTED_MODEL_NAME,\n",
    "    model_alias=\"latest\",\n",
    "    with_gpu=True,  # Use GPU for inference\n",
    ")\n",
    "\n",
    "print(f\"ðŸš€ Deployment job started!\")\n",
    "print(f\"Job ID: {job.id}\")\n",
    "print(f\"Status: {job.status}\")\n",
    "print(f\"Model: {job.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4828d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check deployment status\n",
    "job = api.deploy.get_by_id(job.id)\n",
    "\n",
    "print(f\"Job Status: {job.status}\")\n",
    "print(f\"Progress: {job.progress_percentage}%\")\n",
    "\n",
    "if job.error_message:\n",
    "    print(f\"Error: {job.error_message}\")\n",
    "    print(f\"Build Logs:\\n{job.build_logs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c339a7",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Congratulations! ðŸŽ‰ You've completed the UNet++ Segmentation Tutorial with the BUSI dataset.\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **Data Management** | Uploaded ultrasound images and masks to Datamint |\n",
    "| **Custom Dataset** | Built a PyTorch Dataset for 2D ultrasound images |\n",
    "| **Model Architecture** | Implemented UNet++ with combined CrossEntropy + Dice loss |\n",
    "| **Training** | Trained with MLflow experiment tracking |\n",
    "| **Deployment** | Built a DatamintModel adapter for production inference |\n",
    "\n",
    "### References\n",
    "\n",
    "- [Datamint Documentation](https://sonanceai.github.io/datamint-python-api/)\n",
    "- [BUSI Dataset](https://www.kaggle.com/datasets/aryashah2k/breast-ultrasound-images-dataset)\n",
    "- [Segmentation Models PyTorch](https://github.com/qubvel/segmentation_models.pytorch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
